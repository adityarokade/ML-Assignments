{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Support Vector Machine(SVM) as a classifier can conveniently perform tasks for both linearly separable and non-linearly separable data points, using its superpower(the kernel trick). This unique algorithm was first introduced in the 1960s and later improved on in the 1990s by Vapnik et al.\n"
     ]
    }
   ],
   "source": [
    "#1. What is the underlying concept of Support Vector Machines?\n",
    "\n",
    "\n",
    "print(\"The Support Vector Machine(SVM) as a classifier can conveniently perform tasks for both linearly separable and non-linearly separable data points, using its superpower(the kernel trick). This unique algorithm was first introduced in the 1960s and later improved on in the 1990s by Vapnik et al.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data points or vectors that are the closest to the hyperplane and which affect the position of the hyperplane are termed as Support Vector. Since these vectors support the hyperplane, hence called a Support vector.\n"
     ]
    }
   ],
   "source": [
    "#2. What is the concept of a support vector?\n",
    "\n",
    "\n",
    "print(\"The data points or vectors that are the closest to the hyperplane and which affect the position of the hyperplane are termed as Support Vector. Since these vectors support the hyperplane, hence called a Support vector.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because Support Vector Machine (SVM) optimization occurs by minimizing the decision vector w, the optimal hyperplane is influenced by the scale of the input features and it's therefore recommended that data be standardized (mean 0, var 1) prior to SVM model training.\n"
     ]
    }
   ],
   "source": [
    "#3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "\n",
    "print(\"Because Support Vector Machine (SVM) optimization occurs by minimizing the decision vector w, the optimal hyperplane is influenced by the scale of the input features and it's therefore recommended that data be standardized (mean 0, var 1) prior to SVM model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What about a probability? üëâ An SVM classifier can give the distance between the test instance and the decision boundary as output, so we can use that as a confidence score, but we cannot use this score to directly converted it into class probabilities.\n"
     ]
    }
   ],
   "source": [
    "#4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
    "\n",
    "\n",
    "print(\"What about a probability? üëâ An SVM classifier can give the distance between the test instance and the decision boundary as output, so we can use that as a confidence score, but we cannot use this score to directly converted it into class probabilities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So if there are millions of instances, you should definitely use the primal form, because the dual form will be much too slow. Say you trained an SVM classifier with an RBF kernel.\n"
     ]
    }
   ],
   "source": [
    "#5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "\n",
    "print(\"So if there are millions of instances, you should definitely use the primal form, because the dual form will be much too slow. Say you trained an SVM classifier with an RBF kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Kernel Radial Basis Function (RBF): Same as above kernel function, adding radial basis method to improve the transformation.\n"
     ]
    }
   ],
   "source": [
    "#6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "\n",
    "\n",
    "print(\"Gaussian Kernel Radial Basis Function (RBF): Same as above kernel function, adding radial basis method to improve the transformation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We just use an additional penalizing factor in the Objective function called ùúâ·µ¢. This factor is the distance exceeded from the respective support hyperplane by a data point towards the other class. Therefore, if the data point is well within the boundary (support hyperplane), the penalizing factor ùúâ·µ¢ is 0. Else if the data point is on the other side, this factor ùúâ·µ¢ is equal to its distance between the datapoint and the support hyperplane. Hence, the value ùúâ·µ¢ is a non-negative number.\n"
     ]
    }
   ],
   "source": [
    "#7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "print(\"We just use an additional penalizing factor in the Objective function called ùúâ·µ¢. This factor is the distance exceeded from the respective support hyperplane by a data point towards the other class. Therefore, if the data point is well within the boundary (support hyperplane), the penalizing factor ùúâ·µ¢ is 0. Else if the data point is on the other side, this factor ùúâ·µ¢ is equal to its distance between the datapoint and the support hyperplane. Hence, the value ùúâ·µ¢ is a non-negative number.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from sklearn import datasets iris = datasets.load_iris() X = iris[‚Äúdata‚Äù][:, (2, 3)] # petal length, petal width y=iris[‚Äútarget‚Äù] #Since Target Variable also has Virginica in it so we‚Äôre just taking #out indexes \n"
     ]
    }
   ],
   "source": [
    "#8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "\n",
    "\n",
    "print(\"from sklearn import datasets iris = datasets.load_iris() X = iris[‚Äúdata‚Äù][:, (2, 3)] # petal length, petal width y=iris[‚Äútarget‚Äù] #Since Target Variable also has Virginica in it so we‚Äôre just taking #out indexes \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
