{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with 1 million instances? (106) ≈ 20 (actually a bit more since the tree will generally not be perfectly well balanced).\n"
     ]
    }
   ],
   "source": [
    "#1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?\n",
    "\n",
    "\n",
    "print(\"What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with 1 million instances? (106) ≈ 20 (actually a bit more since the tree will generally not be perfectly well balanced).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A node's Gini impurity is generally lower than that of its parent as the CART training algorithm cost function splits each of the nodes in a way that minimizes the weighted sum of its children's Gini impurities.\n"
     ]
    }
   ],
   "source": [
    "#2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?\n",
    "\n",
    "print(\"A node's Gini impurity is generally lower than that of its parent as the CART training algorithm cost function splits each of the nodes in a way that minimizes the weighted sum of its children's Gini impurities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over-fitting is the phenomenon in which the learning system tightly fits the given training data so much that it would be inaccurate in predicting the outcomes of the untrained data. In decision trees, over-fitting occurs when the tree is designed so as to perfectly fit all samples in the training data set.\n"
     ]
    }
   ],
   "source": [
    "#3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?\n",
    "\n",
    "\n",
    "print(\"Over-fitting is the phenomenon in which the learning system tightly fits the given training data so much that it would be inaccurate in predicting the outcomes of the untrained data. In decision trees, over-fitting occurs when the tree is designed so as to perfectly fit all samples in the training data set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features? Decision Trees don't care whether or not the training data is scaled or centered; scaling the input features will just be a waste of time.\n"
     ]
    }
   ],
   "source": [
    "#4. Explain if its a  good idea to try scaling the input features if a Decision Tree underfits the training set?\n",
    "\n",
    "\n",
    "print(\"If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features? Decision Trees don't care whether or not the training data is scaled or centered; scaling the input features will just be a waste of time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, when we multiplied the size of the training set by 10, then the training time will be multiplied by some factor, say K. For 10 million instances i.e., m = 106, then we get the value of K ≈ 11.7. Therefore, we can expect the training time to be roughly 11.7 hours.\n"
     ]
    }
   ],
   "source": [
    "#5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?\")\n",
    "\n",
    "\n",
    "print(\"So, when we multiplied the size of the training set by 10, then the training time will be multiplied by some factor, say K. For 10 million instances i.e., m = 106, then we get the value of K ≈ 11.7. Therefore, we can expect the training time to be roughly 11.7 hours.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. If your training set contains 100,000 instances, will set presort=True speed up training? Presorting the training set speeds up training only if the dataset is smaller than a few thousand instances. If it contains 100,000 instances, setting presort=True will considerably slow down the training.\n"
     ]
    }
   ],
   "source": [
    "#6. Will setting presort=True speed up training if your training set has 100,000 instances?\n",
    "\n",
    "\n",
    "print(\"6. If your training set contains 100,000 instances, will set presort=True speed up training? Presorting the training set speeds up training only if the dataset is smaller than a few thousand instances. If it contains 100,000 instances, setting presort=True will considerably slow down the training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to tune this is to plot the decision tree and look into the gini index. Interpreting a decision tree should be fairly easy if you have the domain knowledge on the dataset you are working with because a leaf node will have 0 gini index because it is pure, meaning all the samples belong to one class.\n"
     ]
    }
   ],
   "source": [
    "#7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "\n",
    "print(\"The best way to tune this is to plot the decision tree and look into the gini index. Interpreting a decision tree should be fairly easy if you have the domain knowledge on the dataset you are working with because a leaf node will have 0 gini index because it is pure, meaning all the samples belong to one class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
